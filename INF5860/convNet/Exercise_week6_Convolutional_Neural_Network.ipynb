{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Convolutional Neural Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement a convolutional neural network (task2). The goal is for you to get experience with selecting:\n",
    "- Number of convolutional layers\n",
    "- Number of filters in each convolutional layer\n",
    "- Choice of activation function\n",
    "- Choice of initialization of the weights\n",
    "- Number of fully connected layers and their size\n",
    "- Learning rate\n",
    "\n",
    "Before starting to design a convolutional neural network, it is important to understand how the theoretical receptive field changes with filter size and stride (pooling). You will calculate the theoretical receptive field for a couple of given network architectures in task1.\n",
    "\n",
    "\n",
    "Links:\n",
    "- [Task1: Calculation of the theoretical receptive field](#Task1)\n",
    "- [Task2: Classification of MNIST Fashion using convolutional neural network](#Task2)\n",
    "\n",
    "\n",
    "Software version:\n",
    "- Python 3.6\n",
    "- TensorFlow 1.4.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Task1'></a>\n",
    "### Task1: Calculation of the theoretical receptive field\n",
    "---\n",
    "\n",
    "You shall implement equation(1) and use it to calculate the receptive field for 5 different convolutional neural network architectures. When evaluating the receptive field, the image size (resolution) and the size of the objects of interest is important to consider.\n",
    "\n",
    "\n",
    "- Receptive field: ùëÖ \n",
    "- Filter size: ùêπ\n",
    "- Stride: ùë†\n",
    "- Layer index: ùëò\n",
    "\n",
    "Equation (1)\n",
    "\n",
    " $$R_k = R_{k-1} + \\bigg[ (F_k -1)\\cdot \\prod_{i=1}^{k-1} s_i      \\bigg] $$\n",
    " \n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"images/receptive_field2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def receptive_field(f, s):\n",
    "    # Inputs:\n",
    "    # f: A list of the filter size for each layer\n",
    "    # s: A list of the stride for each layer\n",
    "    \n",
    "    # Output\n",
    "    # R: The calculated receptive field for each layer as a list\n",
    "    \n",
    "    # To do: \n",
    "    # Implement equation(1)\n",
    "    R = None\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the architectures\n",
    "\n",
    "# Architecture1\n",
    "A1_filterSize = [3, 3, 3, 3, 3, 3]\n",
    "A1_stride     = [1, 1, 1, 1, 1, 1]\n",
    "A1_Recept     = receptive_field(A1_filterSize, A1_stride)\n",
    "\n",
    "# Architecture2\n",
    "A2_filterSize = [3, 3, 3, 3, 3, 3]\n",
    "A2_stride     = [2, 1, 2, 1, 2, 1]\n",
    "A2_Recept     = receptive_field(A2_filterSize, A2_stride)\n",
    "\n",
    "# Architecture3\n",
    "A3_filterSize = [3, 3, 3, 3, 3, 3]\n",
    "A3_stride     = [2, 2, 2, 2, 2, 2]\n",
    "A3_Recept     = receptive_field(A3_filterSize, A3_stride)\n",
    "\n",
    "# Architecture4\n",
    "A4_filterSize = [5, 5, 5, 5, 5, 5]\n",
    "A4_stride     = [1, 1, 1, 1, 1, 1]\n",
    "A4_Recept     = receptive_field(A4_filterSize, A4_stride)\n",
    "\n",
    "# Architecture5\n",
    "A5_filterSize = [5, 5, 5, 5, 5, 5]\n",
    "A5_stride     = [2, 1, 2, 1, 2, 1]\n",
    "A5_Recept     = receptive_field(A5_filterSize, A5_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "plt.plot(np.array(A1_Recept), 'r', label='Architecture 1')\n",
    "plt.plot(np.array(A2_Recept), 'b', label='Architecture 2')\n",
    "plt.plot(np.array(A3_Recept), 'g', label='Architecture 3')\n",
    "plt.plot(np.array(A4_Recept), 'k', label='Architecture 4')\n",
    "plt.plot(np.array(A5_Recept), 'm', label='Architecture 5')\n",
    "plt.ylabel('Receptive field  (R)', fontsize=18)\n",
    "plt.xlabel('Layer $k$', fontsize=18)\n",
    "ax.grid()\n",
    "plt.ylim([0, 140])\n",
    "plt.xlim([0, 6])\n",
    "ax.legend(loc='upper left', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Task2'></a>\n",
    "### Task2: Classification of MNIST Fashion using convolutional neural network\n",
    "\n",
    "---\n",
    "You will implement a convolution neural network to classify the MNIST fashion dataset. The MNIST fashion dataset includes 10 classes: \n",
    "- ['T-shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'].  \n",
    "\n",
    "The training set consists of 60,000 images and the test set consists of 10,000 images. The images are gray scale and of size [28,28,1]. Before you start, you need to download the MNIST fashion dataset. This can be done by running \"get_datasets.sh\" found in the \"dataset\" folder, or by downloading the dataset from https://github.com/zalandoresearch/fashion-mnist (remember to move the downloaded files into the \"dataset\" folder).\n",
    "\n",
    "\n",
    "Your main task is to fill in the missing code in the following functions:\n",
    "  - convLayer2D()\n",
    "  - flatten()\n",
    "  - fullyConnectedLayer()\n",
    "  \n",
    "\n",
    "You will also need to define the network architecture yourself. That is done by modifying the lists:\n",
    "- numbOfFilters \n",
    "- strides       \n",
    "- kernelSizes \n",
    "- hiddenLayerSizes\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- <b>\"%matplotlib inline\"</b> is used to plot figures within the jupyter notebook\n",
    "- <b>\"tf.reset_default_graph()\"</b> is added to clear the TensorFlow graph from any previous nodes/operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import dataClass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "Implement the convolutional layer function, convLayer2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convLayer2D(data, filters_out=16, kernelSize=(3,3), stride=(1,1), name=None):\n",
    "    # Input parameters:\n",
    "    # - \"data\"         : Input data in format [N, Ny, Nx, C]. N=batch size, Ny/Nx=spatial dimension, C=number of channels\n",
    "    # - \"filters_out\"  : The number of filers (channel dimension)\n",
    "    # - \"kernelSize\"   : The kernel size in dimension Ny and Nx \n",
    "    # - \"stride\"       : The stride in dimension Ny and Nx \n",
    "    # - \"name\"         : The variable scope of the conv layer\n",
    "\n",
    "    # Output parameters:\n",
    "    # - \"conv\": Output of the convolutional layer\n",
    "\n",
    "    # To do:\n",
    "    # Implement the convolutional layer. The layer should include biases and an activation function. \n",
    "    # You shall use tf.nn.conv2d, and you cannot use tf.layer.conv etc\n",
    "    conv = None\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The data has to be flattened after the last convolutional layer and before the first fully connected layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(data):\n",
    "    # Input: A tensor of shape [N, Ny, Nx, C]: N=batch size, Ny/Nx=spatial dinention, C=number of channels\n",
    "    # Output: A flattened version of \"data\" with shape [N, Ny*Nx*C]\n",
    "    data = None\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Implement a fully connected layer using tf.matmul()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fullyConnectedLayer(data, hiddenLayerSize, useActivationFunc=False):\n",
    "    # N -> number of training samples\n",
    "    # D1 -> number of input features\n",
    "    # D2 -> number of output features    \n",
    "    \n",
    "    # Inputs:\n",
    "    # data: Shape [N, D1] \n",
    "    # hiddenLayerSize: int D2\n",
    "    \n",
    "    #Output:\n",
    "    # the output shall have the shape [N,D2]\n",
    "    a = None\n",
    "    if useActivationFunc==True:\n",
    "        # use an activation function\n",
    "\n",
    "    else:\n",
    "        # Do not use an activation function\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The following cell creates an instance of the class dataClass. The instance \"myData\" loads all the MNIST fashion images. The images are of size $[28,28,1]$. The dataClass have useful functions:\n",
    "- next_training_batch(batch_size)\n",
    "- get_test_data()\n",
    "\n",
    "\n",
    "To be able to feed the training and the test data into the tensorflow graph, we define tf.placeholders for the data and the corresponding labels (onehot format). The \"global_step\" variable will be used to count the number of training iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load MNIST fashion data\n",
    "datapath = 'datasets/'\n",
    "myData      = dataClass.dataClassCNN(datapath)\n",
    "\n",
    "#Define placeholders for being able to feed data to the tensorflow graph\n",
    "data          = tf.placeholder(shape=(None, myData.numbOfFeatures[0],myData.numbOfFeatures[1],myData.numbOfFeatures[2]), dtype=tf.float32, name='data')\n",
    "labels_onehot = tf.placeholder(shape=(None, myData.numbOfClasses),  dtype=tf.int32,   name='labels_onehot')\n",
    "global_step   = tf.Variable(initial_value=0, trainable=False, name='global_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['T-shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "plt.figure(figsize=(18, 16), dpi= 80)\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(myData.y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(myData.X_train[idx, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The number of convolutional layers in the network is given by the length of the lists: numbOfFilters, strides and kernalSizes. To improve the classification accuracy you will need to modify the network architecture. Try to add convolutional layers and test with different strides, kernel sizes and number of filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#numbOfFilters = [32, 32]\n",
    "#strides       = [ [1,1], [1,1]]\n",
    "#kernelSizes   = [ [3,3], [3,3] ]\n",
    "\n",
    "conv = data\n",
    "for ii in range(len(numbOfFilters)):\n",
    "    layerName = 'convLayer%s' % ii\n",
    "    conv = convLayer2D(conv, filters_out=numbOfFilters[ii], kernelSize=kernelSizes[ii], stride=strides[ii], name=layerName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Convolutional neural networks often includes fully connected layers at the end. In the next cell you can specify the number of fully connected layers and their sizes by changing the list \"hiddenLayerSizes\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Flatten the data\n",
    "conv = flatten(conv)\n",
    "\n",
    "# Define the fully connected layers:\n",
    "hiddenLayerSizes = [64, myData.numbOfClasses]\n",
    "a = conv\n",
    "for ii in range(len(hiddenLayerSizes)):\n",
    "    layerName = 'FullyConnectedlayer%s' % ii\n",
    "    with tf.variable_scope(layerName):\n",
    "        if ii < len(hiddenLayerSizes) - 1:\n",
    "            a = fullyConnectedLayer(a, hiddenLayerSizes[ii], useActivationFunc=True)\n",
    "        else:\n",
    "            a = fullyConnectedLayer(a, hiddenLayerSizes[ii])\n",
    "logits = a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The loss is computed using the built in tensorflow function \"tf.losses.softmax_cross_entropy\". It calculates the softmax cross  entropy loss. If you want to improve the generalization of the network, you could try to add regularization loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define your loss function\n",
    "loss    = tf.losses.softmax_cross_entropy(onehot_labels=labels_onehot, logits=logits)\n",
    "regloss = 0\n",
    "losses  = loss + regloss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We define a gradient descent optimizer. We pass in the loss (losses) we want to minimize, and the list of the variables (weights) we want to minimize the loss with respect to. The minimizer returns an operation which we call, \"train_op\". Every time we want to perform a gradient descent step we will call <b>\"train_op\"</b> in the tf.Session.\n",
    "\n",
    "The <b>\"global_step\"</b> variable is passed into the minimizer and is incremented for every gradient descent step.\n",
    "\n",
    "We would like you to play with the learning_rate. Default <b>\"learning_rate=0.05\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define an optimizer\n",
    "all_variables = tf.trainable_variables()\n",
    "optimizer     = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "train_op      = optimizer.minimize(losses, global_step=global_step, var_list=all_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The accuracy measure <b>\"accuracy\"</b> is calculated. Other possible measures could be: recall, precision, f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate the accuracy\n",
    "estimated_class = tf.argmax(logits, axis=1)\n",
    "labels          = tf.argmax(labels_onehot, axis=1)\n",
    "accuracy        = tf.reduce_mean(tf.cast(tf.equal(estimated_class, labels), tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here is where the action takes place! The cell creates a <b>\"tf.Session\"</b> and trains the neural network by calling the <b>\"train_op\"</b>. Note, see how we use the two placeholders <b>\"data\"</b> and <b>\"labels_onehot\"</b> to feed the graph with new training images/labels. If training takes a long time try to reduce <b>\"numbOfTrainingSteps\"</b>.\n",
    "\n",
    "You can try to play with the hyperparameters:\n",
    "- numbOfTrainingSteps\n",
    "- batch_size \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "numbOfTrainingSteps = 6000\n",
    "batch_size          = 500\n",
    "\n",
    "#Log train loss/accuracy and test loss/accuracy\n",
    "train_loss     = np.zeros(numbOfTrainingSteps)\n",
    "train_accuracy = np.zeros(numbOfTrainingSteps)\n",
    "test_loss     = []\n",
    "test_accuracy = []\n",
    "test_inds     = []\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    timeZero = time.time()\n",
    "    for ii in range(numbOfTrainingSteps):\n",
    "        npData, npLabels_onehot   = myData.next_training_batch(batch_size)\n",
    "        loss_val, accuracy_val, _ = sess.run([loss, accuracy, train_op],\n",
    "                                             feed_dict={data: npData, labels_onehot: npLabels_onehot})\n",
    "        train_loss[ii]         = loss_val\n",
    "        train_accuracy[ii]     = accuracy_val\n",
    "\n",
    "        #Block is printing accuracy, loss and ETA.\n",
    "        if ii % 50 == 0:\n",
    "            currentTime = time.time()-timeZero\n",
    "            secPerIter  = currentTime/(ii+1)\n",
    "            remTime     = (numbOfTrainingSteps - ii)*secPerIter\n",
    "            remMin      = int(remTime/60)\n",
    "            remSec      = remTime%60\n",
    "            print('%0.2f | Accuracy=%f | loss=%f | ETA: min=%d, sec=%d' % ((ii/numbOfTrainingSteps), train_accuracy[ii], train_loss[ii], remMin, remSec))\n",
    "        \n",
    "        #Block is calculating test accuracy and loss\n",
    "            if ii % 400 == 0:\n",
    "                loss_avg_val     = 0\n",
    "                accuracy_avg_val = 0\n",
    "                testItr = int(np.ceil(myData.numbOfTestSamples/myData.test_batch_size))\n",
    "                for tt in range(testItr):\n",
    "                    npData, npLabels_onehot = myData.get_test_data()\n",
    "                    loss_val, accuracy_val = sess.run([loss, accuracy], feed_dict={data: npData, labels_onehot: npLabels_onehot})\n",
    "                    loss_avg_val += loss_val\n",
    "                    accuracy_avg_val += accuracy_val\n",
    "                test_loss.append(loss_avg_val/testItr)\n",
    "                test_accuracy.append(accuracy_avg_val/testItr)\n",
    "                test_inds.append(ii)\n",
    "                printStr = '%0.2f | Test  Accuracy: %0.3f | Test Loss: %0.5f -----------' % ((ii/numbOfTrainingSteps), accuracy_avg_val/testItr, loss_avg_val/testItr)\n",
    "                print(printStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We plot the loss and the accuracy as a function of gradient descent steps to monitor the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the training accuracy and the training loss\n",
    "#plt.figure()\n",
    "plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "# plt.subplots_adjust(hspace=2)\n",
    "ax.plot(train_loss, 'b', label='train_loss')\n",
    "ax.plot(np.array(test_inds), np.array(test_loss), 'r', label='test_loss')\n",
    "ax.grid()\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "plt.xlabel('Iterations', fontsize=18)\n",
    "ax.legend(loc='upper right', fontsize=16)\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "ax.plot(train_accuracy, 'b', label='train_accuracy')\n",
    "ax.plot(np.array(test_inds), np.array(test_accuracy), 'r', label='test_accuracy')\n",
    "ax.grid()\n",
    "plt.ylabel('Accuracy', fontsize=18)\n",
    "plt.xlabel('Iterations', fontsize=18)\n",
    "max_test_acc_val = np.max(np.array(test_accuracy))\n",
    "ind_max_test_acc_val = test_inds[np.argmax(np.array(test_accuracy))]\n",
    "plt.axvline(x=ind_max_test_acc_val, color='g', linestyle='--', label='Best test accuracy')\n",
    "plt.title('Best test accuracy = %0.1f %%' % (max_test_acc_val*100), fontsize=16)\n",
    "ax.legend(loc='lower right', fontsize=16)\n",
    "plt.ion()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
