{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory assignment 2\n",
    "\n",
    "INF5860 / INF9860 - Machine Learning for Image Analysis  \n",
    "University of Oslo  \n",
    "Spring 2018  \n",
    "  \n",
    "  \n",
    "Handout: 02.03.2018  \n",
    "Delivery deadline: 23.03.2018, 23:59]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classification with deep learning and dense neural networks\n",
    "\n",
    "In this exercise, you are supposed to implement a basic neural network for image classification. The network will be densly connected, with an arbitrary number of layers, and number of nodes in each layer. We shall implement a ReLu activation function, and use a softmax activation in the last layer. The error will be measured with a cross-entropy loss function, and the cost will be minimized using a stochastic gradient descent optimization routine.\n",
    "\n",
    "\n",
    "### Evaluation format\n",
    "\n",
    "You will be guided through the implementation step by step, and you can check your implementation at each step. Each subtask will be judged independently, so it should be possible to do one task even if you have not succeded in the previous. Note, however, that each step needs to be correct in order for the whole method to work at the end.\n",
    "\n",
    "### Exercise content\n",
    "\n",
    "- All subtasks that you are to answer is found in this notebook.\n",
    "- In addition, there is a `src` folder that contains a skeleton for the classifier, including a complete way to import data.\n",
    "- All implementation should be done in the respective files in the `src` folder\n",
    "\n",
    "```\n",
    "def implement_this_function(argument_1):\n",
    "    \"\"\"This is an illustrative dummy function\"\"\"\n",
    "    # TODO: Task X.Y\n",
    "    result = None\n",
    "    \n",
    "    return result\n",
    "```\n",
    "- Some function headers are already given, and necessary, as they are called by the subtasks in this notebook.\n",
    "- Everything else you feel you need to implement, you can implement as you like.\n",
    "- When you have implemented everything (correctly), you should be able to run the whole classifier as `python src/main.py`\n",
    "- Suggestion about the values of different hyperparameters will be given, but you are encouraged to experiment in the final subtask.\n",
    "\n",
    "### What you should implement\n",
    "\n",
    "The skeleton of this program that is already implemented contains things such as:\n",
    "- Program setup\n",
    "- Configurations\n",
    "- Data import of three datasets: mnist, cifar10, and svhn\n",
    "- Training framework\n",
    "- Evaluation framework\n",
    "\n",
    "You should implement the content in the training framework. All steps will be given as tasks and subtasks below. The following are *you* supposed to implement.\n",
    "1. Parameter initialization\n",
    "2. Forward propagation through a network with *arbitrary number of layer* where each layer has an *arbitrary number of nodes*\n",
    "  1. ReLu activation function\n",
    "  2. Softmax function\n",
    "  3. The rest of the forward propagation\n",
    "3. Cross Entropy cost function\n",
    "4. Backward propagation through network with *arbitrary number of layer* where each layer has an *arbitrary number of nodes*\n",
    "  1. Derivative of the ReLu activation function\n",
    "  2. The rest of the backwar propagation\n",
    "5. Parameter update using Gradient Descent optimization\n",
    "6. Run the finished method\n",
    "  1. Reproduce result with default settings\n",
    "  2. Exceed the default result by experimenting with different hyperparameter configurations.\n",
    "\n",
    "\n",
    "### Additional notes\n",
    "\n",
    "Most variables should be self-explanatory, but there are four important dictionaries worth mentioning, as they will control the data flow of the entire program\n",
    "\n",
    "- `conf`: Contains all configurations of the program. These configurations will be passed around most functions, even though we most often will only need a couple of them; this is so that you are freer to experiment outside the boundaries of the program skeleton. They are set with some default values in the `config()` function in `src/main.py`.\n",
    "- `params`: Contains all trainiable parameters, that is, all weight and bias arrays.\n",
    "- `grads`: Contains the gradients of the respective trainable parameters.\n",
    "- `features`: Contains input and output data, in addition to linear combination arrays `Z` and activation arrays `A`.\n",
    "\n",
    "It is *strongly* encouraged to implement the vectorized version of things, otherwise, things are to slow.\n",
    "\n",
    "This should be it. Let us begin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def compare_result(expected, proposed, name):\n",
    "    \"\"\"Compares the expected result agains the proposed result and prints some information\"\"\"\n",
    "    if proposed is None:\n",
    "        print(\"The {} does not seem to be implemented yet.\".format(name))\n",
    "        return\n",
    "    if np.allclose(expected, proposed):\n",
    "        print(\"Correct {}!\".format(name))\n",
    "    else:\n",
    "        print(\"This does not seem entirely correct.\")\n",
    "        print(\"This could mean that there is something wrong with the implementation of {}.\".format(name))\n",
    "        print(\"The difference between expected and proposed result is\")\n",
    "        print(expected - proposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Parameter initialization\n",
    "\n",
    "The function you are to implement is `initialization(layer_dimensions)`, located in `src/model.py`. The parameters shall have the following shape\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    W^{[l]} &\\in \\mathbb{R}^{n^{[l-1]}\\times n^{[l]}} \\\\\n",
    "    b^{[l]} &\\in \\mathbb{R}^{n^{[l]}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and have the following values\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    W_{jk}^{[l]} &\\sim \\mathcal{N}\\left(0, \\frac{2}{n^{[l-1]}}\\right) \\\\\n",
    "    b_k^{[l]} &= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "for all $j = 1, \\ldots, n^{[l-1]}$, $k = 1, \\ldots, n^{[l]}$, $l = 1, \\ldots, L$. Here $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$ means that $x$ is sampled from a normal (or gaussian) distribution with mean $\\mu$ and variance $\\sigma^2$. In order to achieve the normal sampling in python, you can use the `numpy.random.normal()` function.  \n",
    "\n",
    "This initialization fits well with ReLu activations, and is proposed in [He et al. (2015)](https://arxiv.org/pdf/1502.01852.pdf). For another common initialization scheme, you can study the paper by [Glorot and Hinton (2010)](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function you have implemented\n",
    "from src.model import initialization\n",
    "\n",
    "# This dummy network has 784 input nodes, two hidden layers with 128 and 32 nodes, respectively, and\n",
    "# an output layer with 10 nodes\n",
    "conf = {'layer_dimensions': [784, 128, 32, 10]}\n",
    "\n",
    "# You should implement this function\n",
    "params = initialization(conf)\n",
    "\n",
    "# Check your results against the expected. Note that since we are dealing with (pseudo) randomness and\n",
    "# small samples, the values of your array may differ some from the expected. But the absolute difference\n",
    "# should not be very much larger than the order of 0.0001 for W_1, 0.001 for W_2, and 0.01 for W_3.\n",
    "for key, value in sorted(params.items()):\n",
    "    print(\"{}\".format(key))\n",
    "    print(\"    Shape = {}\".format(value.shape))\n",
    "    if key[0] == \"W\":\n",
    "        n_lm1 = value.shape[0]\n",
    "        print(\"    Mean: {0:>9.6f} vs {1:>9.6f} Abs diff: {2:>9.6f}\".format(np.mean(value),\n",
    "                                                                            0.0,\n",
    "                                                                            np.abs(np.mean(value) - 0.0)))\n",
    "        print(\"     Std: {0:>9.6f} vs {1:>9.6f} Abs diff: {2:>9.6f}\".format(np.std(value),\n",
    "                                                                            np.sqrt(2/n_lm1),\n",
    "                                                                            np.abs(np.std(value) - np.sqrt(2/n_lm1))))\n",
    "    else:\n",
    "        print(\"     Min: {} vs {}\".format(np.min(value), 0.0))\n",
    "        print(\"     Max: {} vs {}\".format(np.max(value), 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Forward propagation\n",
    "\n",
    "In this task, you shall implement a ReLu activation function and a softmax function, in addition to the rest of the forward propagation. You must figure out yourself when and how to use the softmax and ReLu functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 a): ReLu activation function\n",
    "\n",
    "The rectified linear unit has the following form\n",
    "\n",
    "$$\n",
    "g(x) =\n",
    "\\begin{cases}\n",
    "  x, &\\quad x \\ge 0 \\\\\n",
    "  0, &\\quad x < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and is supposed to be used as an activation function in all nodes, in all hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function you have implemented\n",
    "from src.model import activation\n",
    "from src.tests import task_2a\n",
    "\n",
    "# First, we load the dummy input, and the expected output\n",
    "input_Z, expected_A = task_2a()\n",
    "\n",
    "# You should implement this function\n",
    "A = activation(input_Z, 'relu')\n",
    "\n",
    "# Check the result\n",
    "compare_result(expected_A, A, \"Relu activation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 b): Softmax function\n",
    "\n",
    "The $k$th element of a softmax function evaluated on a vector $x \\in \\mathbb{R}^n$ is given by\n",
    "\n",
    "$$\n",
    "s(x)_k = \\frac{e^{x_k}}{\\sum_{j=1}^n e^{x_j}}\n",
    "$$\n",
    "\n",
    "and is supposed to be used on the linear activations in the last layer. When this function is to be used on \"real data\", it is adviced to implement the \"tricks\" introduced in lecture 4, to guard against numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function you have implemented\n",
    "from src.model import softmax\n",
    "from src.tests import task_2b\n",
    "\n",
    "# First, we load the dummy input and the expected output\n",
    "input_Z, expected_S = task_2b()\n",
    "\n",
    "# You should implement this function. Note that we often also use Å¶ to symbolise the output of the network.\n",
    "S = softmax(input_Z)\n",
    "\n",
    "# Check the result\n",
    "compare_result(expected_S, S, \"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 c): Forward propagation\n",
    "\n",
    "In this task, you shall implement the forward propagation, from data input to softmax output. This means that you will need the above two functions. This also means that the correctness of this task is dependent on the correctness of the relu and softmax task above. This is a bit unfortunate, as this should ideally be a standalone task, but I believe that the ReLu and softmax functions are simple enough that this should not be a very large problem.\n",
    "\n",
    "The forward propagation should be able to handle an arbitrary number of layers and nodes, determined by the `conf['layer_dimensions']` parameter. It should also handle a batch of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function you have implemented\n",
    "from src.model import forward\n",
    "from src.tests import task_2c\n",
    "\n",
    "# We load the input and expected output\n",
    "conf, X_batch, params, expected_Z_1, expected_A_1, expected_Z_2, expected_Y_proposed = task_2c()\n",
    "\n",
    "# Implement this function yourself\n",
    "Y_proposed, features = forward(conf, X_batch, params, is_training=True)\n",
    "\n",
    "compare_result(expected_Z_1, features['Z_1'], \"feature Z_1\")\n",
    "compare_result(expected_A_1, features['A_1'], \"feature A_1\")\n",
    "compare_result(expected_Z_2, features['Z_2'], \"feature Z_2\")\n",
    "compare_result(expected_Y_proposed, Y_proposed, \"proposed Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Cross Entropy cost function\n",
    "\n",
    "Given a batch of predicted outputs $\\hat{Y} = (\\hat{y}_1, \\ldots, \\hat{y}_m)$ and a batch of reference (one-hot encoded) outputs $\\tilde{Y} = (\\tilde{y}_1, \\ldots, \\tilde{y}_m)$, such that\n",
    "$\\hat{y}^{(i)}, \\tilde{y}^{(i)} \\in \\mathbb{R}^n$ for $i = 1, \\ldots, m$, the cross entropy cost is given by\n",
    "\n",
    "$$\n",
    "C(\\hat{Y}, \\tilde{Y}) = -\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^{n} \\tilde{y}_k^{(i)}\\log \\hat{y}_k^{(i)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function you have implemented\n",
    "from src.model import cross_entropy_cost\n",
    "from src.tests import task_3\n",
    "\n",
    "# Generate some dummy input and corresponding expected output\n",
    "Y_proposed, Y_batch, expected_cost_value, expected_num_correct = task_3()\n",
    "\n",
    "# You should implement this function\n",
    "cost_value, num_correct = cross_entropy_cost(Y_proposed, Y_batch)\n",
    "\n",
    "compare_result(expected_cost_value, cost_value, \"cost value\")\n",
    "compare_result(expected_num_correct, num_correct, \"number of correct predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Backward propagation\n",
    "\n",
    "In this task, you shall compute the gradients of the trainable parameters with respect to the cost value. In order to compute them, we will need the following equations.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\nabla_{W^{[l]}} \\mathcal{C} &= \\frac{1}{m} A^{[l-1]} \\mathcal{J}_{z^{[l]}}{(\\mathcal{C})}^\\intercal \\\\\n",
    "  \\nabla_{b^{[l]}} \\mathcal{C} &= \\frac{1}{m}\\left( \\mathcal{J}_{z^{[l]}}{(\\mathcal{C})} \\right) \\mathbf{1}(m) \\\\\n",
    "  \\mathcal{J}_{z^{[l]}}{(\\mathcal{C})}&=g'(Z^{[l]})\\circ \\left(W^{[l+1]} \\mathcal{J}_{z^{[l+1]}}{(\\mathcal{C})}\\right) \\\\\n",
    "  \\mathcal{J}_{z^{[L]}}{(\\mathcal{C})} &= \\hat{Y} - \\tilde{Y}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "I refer to the [lecture slides](https://www.uio.no/studier/emner/matnat/ifi/INF5860/v18/undervisningsmateriale/lectures/slides_inf5860_s18_week04.pdf) for the derivation and explanation of the different terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 a): Derivative of the activation function\n",
    "\n",
    "In order to compute the parameter gradients, we need a function to compute the derivative of the relu activation function. If we take some freedoms, we can use the Heaviside step-function as the derivative of the ReLu activation\n",
    "\n",
    "$$\n",
    "g'(x) =\n",
    "\\begin{cases}\n",
    "  1, &\\quad x \\ge 0 \\\\\n",
    "  0, &\\quad x < 0.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your function\n",
    "from src.model import activation_derivative\n",
    "from src.tests import task_4a\n",
    "\n",
    "# Load dummy input and expected output\n",
    "input_Z, expected_dg_dz = task_4a()\n",
    "\n",
    "# You should implement this\n",
    "dg_dz = activation_derivative(input_Z, \"relu\")\n",
    "\n",
    "# Compare your result with the expected\n",
    "compare_result(expected_dg_dz, dg_dz, \"activation function derivative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 b): Backward propagation\n",
    "\n",
    "Use the equations above, and the function you implemented in Task 4 a) to compute all necessary parameter gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your function\n",
    "from src.model import backward\n",
    "from src.tests import task_4b\n",
    "\n",
    "# Load dummy input and expected output\n",
    "(conf, Y_proposed, Y_batch, params, features,\n",
    " expected_grad_W_1, expected_grad_b_1, expected_grad_W_2, expected_grad_b_2) = task_4b()\n",
    "\n",
    "# This is the function that you shall implement\n",
    "grad_params = backward(conf, Y_proposed, Y_batch, params, features)\n",
    "\n",
    "# Check your implementation\n",
    "compare_result(expected_grad_W_1, grad_params['grad_W_1'], \"gradient of cost w.r.t. W_1\")\n",
    "compare_result(expected_grad_b_1, grad_params['grad_b_1'], \"gradient of cost w.r.t. b_1\")\n",
    "compare_result(expected_grad_W_2, grad_params['grad_W_2'], \"gradient of cost w.r.t. W_2\")\n",
    "compare_result(expected_grad_b_2, grad_params['grad_b_2'], \"gradient of cost w.r.t. b_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Update parameters\n",
    "\n",
    "Given some learning rate $\\lambda \\in \\mathbb{R}$, the gradient descent update is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  w_{jk}^{[l]} &\\gets w_{jk}^{[l]} - \\lambda \\frac{\\partial \\mathcal{C}}{\\partial w_{jk}^{[l]}} \\\\\n",
    "  b_k^{[l]}    &\\gets b_k^{[l]} - \\lambda \\frac{\\partial \\mathcal{C}}{\\partial b_k^{[l]}}\n",
    "  \\end{align}\n",
    "$$\n",
    "\n",
    "for all\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "  j &= 1, \\ldots, n^{[l-1]} \\\\\n",
    "  k &= 1, \\ldots, n^{[l]} \\\\\n",
    "  l &= 1, \\ldots, L\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your function\n",
    "from src.model import gradient_descent_update\n",
    "from src.tests import task_5\n",
    "\n",
    "# Load dummy input and expected output\n",
    "(conf, params, grad_params,\n",
    " expected_updated_W_1, expected_updated_b_1, expected_updated_W_2, expected_updated_b_2) = task_5()\n",
    "\n",
    "# You should implement this function\n",
    "updated_params = gradient_descent_update(conf, params, grad_params)\n",
    "\n",
    "# Check your results\n",
    "compare_result(expected_updated_W_1, updated_params['W_1'], \"update of W_1\")\n",
    "compare_result(expected_updated_b_1, updated_params['b_1'], \"update of b_1\")\n",
    "compare_result(expected_updated_W_2, updated_params['W_2'], \"update of W_2\")\n",
    "compare_result(expected_updated_b_2, updated_params['b_2'], \"update of b_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Putting everything together\n",
    "\n",
    "Now it is time to test our implementation on some data. In `src/import_data.py` there is automated scripts for importing the following datasets\n",
    "\n",
    "- MNIST\n",
    "- CIFAR10\n",
    "- SVHN\n",
    "\n",
    "These are chosen because they contain small images, and therefore easy to experiment with. Below, you will find results from training sessions on all datasets using the default configuration.\n",
    "\n",
    "**Default configuration**\n",
    "\n",
    "```\n",
    "hidden_layers = [128, 32]\n",
    "devel_size = 5000\n",
    "activation_function = 'relu'\n",
    "batch_size = 128\n",
    "learning_rate = 1.0e-2\n",
    "```\n",
    "\n",
    "### How to report results\n",
    "\n",
    "The most important is that you are able to present your findings in an understandable way. It is most convenient for everyone that you report your results in this notebook. Below follows three different suggestions of ways to do this.\n",
    "\n",
    "**Method 1: Running program and manually copy/paste results here**\n",
    "\n",
    "This is done in the examples below, and is nice because of the danger of overwriting things by accident is smaller. But beware that the notebook kernel often needs to be restarted (or similar) before changes are noticed.\n",
    "\n",
    "To do this, some markdown formatting is worth explaining. You can find the jupyter notebook markdown cell specification at [here](http://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html)\n",
    "Images can be linked to as follows\n",
    "\n",
    "```\n",
    "![caption name](relative/path/to/image.png)\n",
    "```\n",
    "\n",
    "and you can create monospaced blocks by enclosing the block with three \"accent grave\" symbols at the top and bottom of the block. The following block"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "```\n",
    "Cell content here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will be rendered as\n",
    "\n",
    "```\n",
    "Cell content here\n",
    "```\n",
    "\n",
    "** Method 2: Running your program inside the notebook**\n",
    "\n",
    "You can import the program here, and run the program. This is possibly the simplest, but be careful not to overwrite results. In order for this to work, you need to put the source directory in the python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "src_dir = os.path.join(os.getcwd(), \"src\")\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main and run\n",
    "from src.main import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Method 3: Running your program inside the notebook**\n",
    "\n",
    "Instead of importing the `main()` function, you can copy/paste the content of `main()` here, and run things manually with the desired config. This is a bit verbose, but is less prone to overwrite existing results. For this method, you need to add the source directory to the python path as shown in **Method 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module\n",
    "\n",
    "import src.main as main\n",
    "import src.run as run\n",
    "\n",
    "# Import config, and do changes if you want to\n",
    "conf = main.config()\n",
    "conf['max_steps'] = 3000\n",
    "\n",
    "X_train, Y_train, X_devel, Y_devel, X_test, Y_test = main.get_data(conf)\n",
    "\n",
    "params, train_progress, devel_progress = run.train(conf, X_train, Y_train, X_devel, Y_devel)\n",
    "\n",
    "main.plot_progress(train_progress, devel_progress)\n",
    "\n",
    "print(\"Evaluating train set\")\n",
    "num_correct, num_evaluated = run.evaluate(conf, params, X_train, Y_train)\n",
    "print(\"CCR = {0:>5} / {1:>5} = {2:>6.4f}\".format(num_correct, num_evaluated,\n",
    "                                                 num_correct/num_evaluated))\n",
    "print(\"Evaluating development set\")\n",
    "num_correct, num_evaluated = run.evaluate(conf, params, X_devel, Y_devel)\n",
    "print(\"CCR = {0:>5} / {1:>5} = {2:>6.4f}\".format(num_correct, num_evaluated,\n",
    "                                                 num_correct/num_evaluated))\n",
    "print(\"Evaluating test set\")\n",
    "num_correct, num_evaluated = run.evaluate(conf, params, X_test, Y_test)\n",
    "print(\"CCR = {0:>5} / {1:>5} = {2:>6.4f}\".format(num_correct, num_evaluated,\n",
    "                                                 num_correct/num_evaluated))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "Below is shown some examples using the default configurations.\n",
    "\n",
    "#### MNIST - 2000 steps\n",
    "\n",
    "![Example training progress](figures/mnist_progress_default.png)\n",
    "\n",
    "```\n",
    "Evaluating train set\n",
    "CCR = 50173 / 55000 = 0.9122\n",
    "Evaluating development set\n",
    "CCR =  4668 /  5000 = 0.9336\n",
    "Evaluating test set\n",
    "CCR =  9158 / 10000 = 0.9158\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR10 - 10000 steps\n",
    "\n",
    "![Example training progress](figures/cifar10_progress_default.png)\n",
    "\n",
    "```\n",
    "Evaluating train set\n",
    "CCR = 22475 / 45000 = 0.4994\n",
    "Evaluating development set\n",
    "CCR =  2336 /  5000 = 0.4672\n",
    "Evaluating test set\n",
    "CCR =  4632 / 10000 = 0.4632\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVHN - 10000 steps\n",
    "\n",
    "![Example training progress](figures/svhn_progress_default.png)\n",
    "\n",
    "```\n",
    "Evaluating train set\n",
    "CCR = 49392 / 68257 = 0.7236\n",
    "Evaluating development set\n",
    "CCR =  3576 /  5000 = 0.7152\n",
    "Evaluating test set\n",
    "CCR = 17691 / 26032 = 0.6796\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 a): Reproduce results\n",
    "\n",
    "On all datasets, try to reproduce the results shown above in the examples, with the same configurations. Because of random initialization, it is not expected that the results are exactly the same, even if the implementation is exactly identical to the one used to produce the examples. Report the results in cells below this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 b): Exceed results\n",
    "\n",
    "Experiment with different configurations and try to beat the classification results above. We expect you to experiment a bit, and to give an effort in improving the results, but you will not be judged by whether you managed to exceed the results or not.\n",
    "\n",
    "Suggested things to edit are listed below. Some are available if you have implemented everything in this assignment correctly, others will need some extra implementation. \n",
    "\n",
    "- Out of the box\n",
    "  - Step length (learning rate)\n",
    "  - Number of layers\n",
    "  - Number of nodes in the layers\n",
    "  - Number of training steps\n",
    "- Things you have to implement yourself\n",
    "  - Activation function\n",
    "  - Parameter initialization\n",
    "  - Optimization routine\n",
    "  - Data standardization\n",
    "  \n",
    "Report your results, in addition to the configuration used, in cells below this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
